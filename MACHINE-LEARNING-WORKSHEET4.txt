                                                 MACHINE LEARNING – WORKSHEET 4



In Q1 to Q8, only one option is correct, Choose the correct option:


1. Which of the following in sklearn library is used for hyper parameter tuning?
    
    A) GridSearchCV()           B) RandomizedCV()
    C) K-fold Cross Validation  D) None of the above
    
    Answer: A) GridSearchCV()
    
    
2. In which of the below ensemble techniques trees are trained in parallel?
   
    A) Random forest      B) Adaboost
    C) Gradient Boosting  D) All of the above
    
    Answer: A) Random forest  
    
    
3. In machine learning, if in the below line of code:
   sklearn.svm.SVC (C=1.0, kernel='rbf', degree=3)
   we increasing the C hyper parameter, what will happen?
    
    A) The regularization will increase B) The regularization will decrease
    C) No effect on regularization      D) kernel will be changed to linear
    
    Answer: A) The regularization will increase 
    
    
4. Check the below line of code and answer the following questions:
   sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2)
   Which of the following is true regarding max_depth hyper parameter?
    
    A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.
    B) It denotes the number of children a node can have.
    C) both A & B
    D) None of the above
    
    Answer: A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.
    
    
5. Which of the following is true regarding Random Forests?
    
    A) It's an ensemble of weak learners.
    B) The component trees are trained in series
    C) In case of classification problem, the prediction is made by taking mode of the class labels predicted by the component trees.
    D) None of the above
    
    Answer: A) It's an ensemble of weak learners
    
    
6. What can be the disadvantage if the learning rate is very high in gradient descent?
    
    A) Gradient Descent algorithm can diverge from the optimal solution.
    B) Gradient Descent algorithm can keep oscillating around the optimal solution and may not settle.
    C) Both of them
    D) None of them
    
    Answer: C) Both of them
    
    
7. As the model complexity increases, what will happen?
    
    A) Bias will increase, Variance decrease  B) Bias will decrease, Variance increase
    C) both bias and variance increase        D) Both bias and variance decrease
    
    Answer: B) Bias will decrease, Variance increase
    
    
8. Suppose I have a linear regression model which is performing as follows:
     Train accuracy = 0.95
     Test accuracy  = 0.75
     Which of the following is true regarding the model?
       
       A) model is underfitting     B) model is overfitting
       C) model is performing good  D) None of the above
        
        Answer: B) model is overfitting
        
      
      
Q9 to Q15 are subjective answer type questions, Answer them briefly.

9. Suppose we have a dataset which have two classes A and B. The percentage of class A is 40% and percentage of class B is 60%. 
   Calculate the Gini index and entropy of the dataset.

    Answer: Here we will add both the classes and consider individual class by total as shown below
    
            Gini index: 1- [(40/100)^2 + (60/80)^2]
                        = 0.48
            
            Gini(40,60)= 0.48
             
             
             Calculating entropy 
             
             formula : entropy = -classA * log2(classA) -classB * log2(classB)
                               = 40/100*log2(40/100) -60/100*log2(60/100)
                       
                       entropy = 0.97




10. What are the advantages of Random Forests over Decision Tree?
    
    Answer: The advantages of random forest are: 

            Random Forest can be used to solve both classification as well as regression problems.

            Random Forest works well with both categorical and continuous variables.

            Random Forest can automatically handle missing values.

            No feature scaling required required incase of Random Forest as it uses rule based approach instead of distance calculation.

            Random Forest can automatically handle missing values.

            Random Forest is usually robust to outliers and can handle them automatically.

            Random Forest algorithm is very stable. Even if a new data point is introduced in the dataset, the overall algorithm is not                 affected much since the new data may impact one tree. 

            Random Forest is comparatively less impacted by noise.




11. What is the need of scaling all numerical features in a dataset? Name any two techniques used for scaling.

    Answer: Machine learning algorithm just sees number 
            if there is a vast difference in the range say few ranging in thousands and few ranging in the tens, 
            and it makes the underlying assumption that higher ranging numbers have superiority of some sort. 
            So these more significant number starts playing a more decisive role while training the model.
            For example The machine learning algorithm works on numbers and does not know what that number represents. 
            A weight of 10 grams and a price of 10 dollars represents completely two different things but for a model as a feature, 
            it treats both as same.
            
            The most common techniques of feature scaling are 1) Normalization
                                                              2) Standardization. 




12. Write down some advantages which scaling provides in optimization using gradient descent algorithm.

    Answer: It is easier to fit into memory due to a single training sample being processed by the network
            It is computationally fast as only one sample is processed at a time
            For larger datasets it can converge faster as it causes updates to the parameters more frequently
            It can benefit from the vectorization which increases the speed of processing all training samples together
            It produces a more stable gradient descent convergence and stable error gradient than stochastic gradient descent




13. In case of a highly imbalanced dataset for a classification problem, is accuracy a good metric to measure the
    performance of the model. If not, why?

    Answer: Imbalanced dataset is primarily in the context of supervised machine learning involving two or more classes.
            Imbalance means that the number of data points available for different the classes is different, If there are 
            two classes, then balanced data would mean 50% points for each of the class.
            
            If there are 60% points for one class and 40% for the other class, it should not cause any significant performance
            degradation. Only when the class imbalance is high. i,e 90% points for one class and 10% for the other, 
            standard optimization or performance measures may not be as effective and it would need modification.

            Accuracy as Metrics is not sufficient to judge the 'Goodness' of a Model when data is hugely Imbalanced 
            that's where ROC curve comes into picture to rescue.
            
            
            

14. What is “f-score" metric? Write its mathematical formula.

    Answer: The F-score, also called the F1-score, is a measure of a model’s accuracy on a dataset. 
            It is used to evaluate binary classification systems, which classify examples into ‘positive’ or ‘negative’.
            The F-score is a way of combining the precision and recall of the model, and it is defined as the harmonic 
            mean of the model’s precision and recall.
            The F-score is commonly used for evaluating information retrieval systems such as many kinds of machine 
            learning models, in natural language processing.
            
            
            Formula : f = tp / (tp+1/2(fp+fn))




15. What is the difference between fit(), transform() and fit_transform()?

    Answer: fit() means to fit the model to the data being provided. This is where the model "learns" from the data.
                   It is used for calculating the initial filling of parameters on the training data and saves them 
                   as an internal objects state

            transform() means to transform the data according to the fitted model. i,e the above fit calculated values 
                        and return modified training data 

            fit_transform() means to do both - Fit the model to the data, then transform the data according to the 
                            fitted model. It joins above two steps. Internally, it just calls first fit() and then 
                            transform() on the same data 
