                                        MACHINE LEARNING – WORKSHEET 11 (LINEAR REGRESSION)
                                        
In Q1 to Q8, only one option is correct, Choose the correct option:

1. What happens to R2 measure if we add a new feature?
    A) remains same 
    B) always increases
    C) may or may not increase 
    D) always decreases
    
    Answer: A) remains same 
    
    
2. The correct relationship between SST, SSR and SSE is given by:
    A) SSR = SST + SSE
    B) SST = SSR + SSE
    C) SSE = SSR – SST 
    D) None of the above
    
    Answe:  B) SST = SSR + SSE
    
    
3. Residuals in regression analysis can be defined as:
    A) difference between the actual value and the predicted value.
    B) difference between the actual value and the mean of predicted value.
    C) difference between the actual value and mean of dependent variable.
    D) None of the above.
    
    Answer: D) None of the above
    
    
4. In a simple linear regression model, if we change the input variable by 1 unit, then how much output variable will change?
    A) By 1 
    B) No Change
    C) By its slope 
    D) None of the above
    
    Answer: C) By its slope 
    
    
5. If the coefficient of determination is equal to 1, then correlation coefficient:
    A) must also be equal to 1 
    B) can be either -1 or 1
    C) can be any value between -1 to 1 
    D) must be -1
    
    Answer:  A) must also be equal to 1 
    
    
6. Which of the following plot is best suited for the linear relationship of continuous variables?
    A) Scatter plot 
    B) Histograms
    C) Pie charts 
    D) All of the above
    
    Answer: A) Scatter plot 
    
    
7. The ratio of MSR/MSE produces:
    A) t-statistics
    B) f-statistics
    C) z-statistics
    D) None of the above.
    
    Answer: B) f-statistics
    
    
8. Which of the following regularizations uses only L2 normalization for its penalty parameter?
    A) Lasso 
    B) Elastic Nets
    C) Ridge
    D) All of the above
    
    Answer: C) Ridge
    
    
In Q9 to Q11, more than one options are correct, Choose all the correct options:

9. Which of the following statement/s are true for best fitted line?
    A) It shows the causal relationship between dependent and independent variables
    B) It shows the positive or negative relation between dependent and independent variables
    C) It always goes through origin
    D) It is a straight line that is the best approximation of the given data sets
    
    
    Answer:  A) It shows the causal relationship between dependent and independent variables
             D) It is a straight line that is the best approximation of the given data sets
    
    
10. Regularizations helps in:
    A) Reducing the training time 
    B) Generalizing the test set
    C) Automatic feature selection
    D) Grouping the data
    
    Answer:  B) Generalizing the test set
    
    
11. Linear regression can be implemented through:
    A) Normal Equation 
    B) Singular Value Decomposition
    C) Parity checks 
    D) nodes
    
    Answer: A) Normal Equation 
            B) Singular Value Decomposition
    
Q12 to Q15 are subjective answer type questions, Answer them briefly.

12. Explain R2 and adjusted R2 metrics?

    Answer: R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's 
            explained by an independent variable or variables in a regression model. Whereas correlation explains the strength of 
            the relationship between an independent and dependent variable, R-squared explains to what extent the variance of one 
            variable explains the variance of the second variable. So, if the R2 of a model is 0.50, 
            then approximately half of the observed variation can be explained by the model's inputs.
            
            The Formula for R-Squared Is
                                            R2=1− Unexplained Variation
                                                  Total Variation
                                                  
            The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.
            The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. 
            It decreases when a predictor improves the model by less than expected by chance. 
                
                                Adjusted R Squared= 1 — [(1 — R Squared) * ((n-1) / (n-p-1))]
            
                                    where,
                                    p = number of independent variables.
                                    n = number of records in the data set.
                                    
                                    
13. Explain the cost function of linear regression?

    Answer: It is a function that measures the performance of a Machine Learning model for given data. Cost Function quantifies 
            the error between predicted values and expected values and presents it in the form of a single real number. 
            Depending on the problem Cost Function can be formed in many different ways. The purpose of Cost Function is to be either:
            
            Minimized - then returned value is usually called cost, loss or error. The goal is to find the values of model parameters
                        for which Cost Function return as small number as possible.
            Maximized - then the value it yields is named a reward. The goal is to find values of model parameters for which returned 
                        number is as large as possible.
    
    
14. Differentiate SSE, SSR and SST.

    Answer: The sum of squares total, denoted SST, is the squared differences between the observed dependent variable and its mean.
            You can think of this as the dispersion of the observed variables around the mean – much like the variance in 
            descriptive statistics.
            sum of squares total
            It is a measure of the total variability of the dataset.
            
            The second term is the sum of squares due to regression, or SSR. 
            It is the sum of the differences between the predicted value and the mean of the dependent variable. 
            Think of it as a measure that describes how well our line fits the data.
            Sum of squares regression
            If this value of SSR is equal to the sum of squares total, it means our regression model captures all the observed
            variability and is perfect. Once again, we have to mention that another common notation is ESS or explained sum of squares.
            
            The last term is the sum of squares error, or SSE. The error is the difference between the observed value and the 
            predicted value.
            Sum of squares error
            We usually want to minimize the error. The smaller the error, the better the estimation power of the regression. 
            Finally, I should add that it is also known as RSS or residual sum of squares.
            
    
15. What are the various evaluation metrics for linear regression?

    Answer: Evaluation metrics are a measure of how good a model performs and how well it approximates the relationship. 
            Let us look at MSE, MAE, R-squared, Adjusted R-squared, and RMSE.
            
            Mean Squared Error (MSE)
            The most common metric for regression tasks is MSE. It has a convex shape. It is the average of the squared difference 
            between the predicted and actual value. Since it is differentiable and has a convex shape, it is easier to optimize.
            MSE penalizes large errors.
            
            Mean Absolute Error (MAE)
            This is simply the average of the absolute difference between the target value and the value predicted by the model. 
            Not preferred in cases where outliers are prominent.
            
            R-squared 
            This metric represents the part of the variance of the dependent variable explained by the independent variables of 
            the model. It measures the strength of the relationship between your model and the dependent variable.
            
            Adjusted R-squared 
            The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.
            The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. 
            It decreases when a predictor improves the model by less than expected by chance.
            
            Root Mean Squared Error (RMSE)
            This is the square root of the average of the squared difference of the predicted and actual value.
            R-squared error is better than RMSE. This is because R-squared is a relative measure while RMSE is an absolute measure 
            of fit (highly dependent on the variables — not a normalized measure).
            

    