                                                   MACHINE LEARNING – WORKSHEET 3
                                                  
Q1 to Q15 are subjective answer type questions, Answer them briefly.

1. Give short description each of Linear, RBF, Polynomial kernels used in SVM.
   Answer:     Linear Kernel is used when the data is Linearly separable. It is one of the most common kernels to be used.
           It can be separated using a single Line. It is mostly used when there are a Large number of Features in a
           particular Data Set. 
               There are a lot of features, for example Text Classification, as each alphabet is a new feature. So we mostly 
           use Linear Kernel in Text Classification. Training a SVM with a Linear Kernel is Faster than with any other Kernel.

                RBF kernel, is a popular kernel function used in various kernelized learning algorithms. It is commonly used 
           in support vector machine classification.



2. R-squared or Residual Sum of Squares (RSS) which one of these two is a better measure of goodness of fit of 
   model in regression and why??
   Answer:    R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage
          of the variance in the dependent variable that the independent variables explain collectively. R-squared measures
          the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale.
              
              A residual sum of squares (RSS) used to measure the amount of variance in a data set that is not explained 
          by a regression model. Regression is a measurement that helps determine the strength of the relationship 
          between a dependent variable and a series of other changing variables or independent variables. The residual sum 
          of squares measures the amount of error remaining between the regression function and the data set. A smaller 
          residual sum of squares figure represents a regression function. 



3. What are TSS (Total Sum of Squares), ESS (Explained Sum of Squares) and RSS (Residual Sum of Squares) 
   in regression. Also mention the equation relating these three metrics with each other.
   Answer: 
           The Total sum of squares(TSS) also denoted as SST, It is the squared differences between the observed
           dependent variable and its mean. It is a measure of the total variability of the dataset.

           Residual Sum of Squares is the second term is the sum of squares, It is the sum of the differences 
           between the predicted value and the mean of the dependent variable. 
           
           Explained Sum of Squares is the last term is the sum of squares, The error is the difference between 
           the observed value and the predicted value. The smaller the error, the better the estimation power of 
           the regression.
            
                               They Related as SST = SSR + SSE 
           


4. What is Gini –impurity index?
   Answer:     Gini index or Gini impurity measures the probability of a particular variable being wrongly 
           classified when it is randomly picked. If all the elements belong to a single class, then it can 
           be called pure. The degree of Gini index varies between 0 and 1
           1. 0 denotes that all elements belong to a certain class or to a existing 1 class
           2. 1 denotes that the elements are randomly distributed across various classes
           3. 0.5 denotes equally distributed elements into some classes
   
   
   
5. Are unregularized decision-trees prone to overfitting? If yes, why?
    Answer:     Over-fitting is the phenomenon in which the learning system tightly fits the given training data
           so much that it would be inaccurate in predicting the outcomes of the untrained data. 
                In decision trees, over-fitting occurs when the data is not regluarized to a standard, Over-fitting 
           occurs when the tree is designed so as to perfectly fit all samples in the training data set. Thus it ends
           up with branches, this effects the accuracy when predicting samples that are not part of the training set.
           over-fitting in decision tree is called pruning which is done after the initial training is complete.
           It can be fixed by segregating the actual training set into two sets: training data set and validation data set.
           Then continue trimming the tree accordingly to optimize the accuracy of the validation data set.



6. What is an ensemble technique in machine learning?
   Answer:    Ensemble methods is a machine learning technique that combines several base models in order to 
          produce one optimal predictive model. Ensemble methods usually produces more accurate solutions than
          any other single model would produce. Ensemble methods are used in order to decrease variance, bias, or 
          improve prediction.
          Ensemble methods can be divided into two groups:
                                            1) sequential ensemble 
                                            2) parallel ensemble 
                                           


7. What is the difference between Bagging and Boosting techniques?
   Answer:   Bagging and Boosting are both ensemble methods in Machine Learning.
   
                                      BAGGING                                               BOOSTING
                                       
                                   Random forest                                        Gradient boosting
                                       
                   Simplest way of combining predictions that                 A way of combining predictions that
                          belong to the same type                                belong to the different types
                          
                   Bagging tries to solve over-fitting problem                    Boosting tries to reduce bias
                   
                         Each model is built independently                  New models are influenced by performance of 
                                                                                       previously built models
                                                    
                 If the classifier is unstable (high variance)             If the classifier is stable and simple (high bias).                                                 then apply bagging                                     the apply boosting




8. what is out-of-bag error in random forests?
   Answer:     Out-of-bag error it is also called out-of-bag estimate. This is a method of measuring the prediction 
          error of random forest and other machine learning models utilizing bagging to sub-sample the data samples,
          Which is used for training. Out-of-bag error is the mean prediction error on each training sample x, using 
          only the trees that did not have x in their bagging sample. Sub-sampling allows one to define an out-of-bag 
          error of the prediction. 



9. What is K-fold cross-validation?
   Answer:     The procedure has a single parameter called k that refers to the number of groups that a given 
          data sample is to be split into , such procedure is often called k-fold cross-validation. When a specific 
          value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 
          10-fold cross-validation. It is a popular method because it is simple to understand. It generally results 
          in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test
          split.



10. What is hyper parameter tuning in machine learning and why it is done?
    Answer:      hyperparameter tuning is set of optimal hyperparameters for a learning algorithm. A hyperparameter is
            a parameter whose value is used to control the learning process. Machine learning model can require 
            different constraints, weights or learning rates to generalize different data patterns. These measures 
            are called hyperparameters. these measures have to be tuned so that the model can optimally solve the 
            machine learning problem.
                Hyperparameters are important because they directly control the behaviour of the training algorithm, 
            Hyperparameters have a significant impact on the performance of the model is being trained. Choosing 
            appropriate hyperparameters plays a crucial role in the success of our architecture. Since it makes a huge
            impact on the learned model.



11. What issues can occur if we have a large learning rate in Gradient Descent?
    Answer: 1) When the learning rate is too large, gradient descent can inadvertently increase.
            2) large learning rate in Gradient Descent increases the training error. 
            3) When the learning rate is too small, training is not only slower, but may become permanently stuck
               with a high training error. 
            4) If your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.
            5) Too large training rate causes drastic changes which leads to divergent behavious.
            6) A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a 
               good starting point on your problem



12. What is bias-variance trade off in machine learning?
    Answer: The goal of any machine learning algorithm is to achieve low bias and low variance. 
            Linear machine learning algorithms often have a high bias but a low variance. Nonlinear machine 
            learning algorithms often have a low bias but a high variance.
            There is no escaping the relationship between bias and variance in machine learning. 
                     Increasing the bias will decrease the variance.
                     Increasing the variance will decrease the bias.
            Finally:         
            Bias is the simplifying assumptions made by the model to make the target function easier to approximate.
            Variance is the amount that the estimate of the target function will change given different training data.
            Trade-off is tension between the error introduced by the bias and the variance.



13. What is the need of regularization in machine learning?
    Answer:      Regularisation is a technique used to reduce the errors by fitting the function appropriately 
            on the given training set and avoid overfitting. Regularization is a technique used in an attempt to 
            solve the overfitting. Model will do good but it is probably overfitting, It will probably have poor 
            prediction and generalization power, it sticks too much to the data and the model has probably learned 
            the background noise while being fit and this isn't acceptable. Here where the regularization technique
            comes in handy.
            
            The commonly used regularisation techniques are :
           
            L1 regularisation: A regression model which uses L1 Regularisation technique is called LASSO regression.

            L2 regularisation: A regression model that uses L2 regularisation technique is called Ridge regression.




14. Differentiate between Adaboost and Gradient Boosting
    Answer:  The basic idea of boosting is to combine several weak learners into a stronger one. We will see 
          the differences between Adaptive Boosting and Gradient Boosting.
    
        Adaptive Boosting: 1) Adaboost is more about voting weights  
                           2) Adaboost increases the accuracy by giving more weightage to the target
                           3) Modifying the weights attached to each of the instances 
                           4) It increases the weights of the wrongly predicted instances and decreases
                              the ones of the correctly predicted instances.
                              
        Gradient boosting: 1) Gradient boosting is more about adding gradient optimization
                           2) Gradient boosting increases the accuracy by minimizing the Loss Function
                           3) Gradient boosting algorithm builds first weak learner and calculates the Loss Function
                           4) It then builds a second learner to predict the loss after the first step and so on until 
                              a certain threshold is reached.

  


15. Can we use Logistic Regression for classification of Non-Linear Data? If not, why?
    Answer:    ogistic regression is a powerful machine learning algorithm that utilizes a sigmoid function i,e 
           A sigmoid function is a bounded, differentiable, real function that is defined for all real input values 
           and has a non-negative derivative at each point. 
               Logistic Regression works best on binary classification problems,
           although it can be used on multi-class classification problems. Logistic regression is not fit for regression tasks.
    
   